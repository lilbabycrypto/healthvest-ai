{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HealthVest AI - Lab Report Analyzer\n",
    "\n",
    "**MedGemma Impact Challenge Submission**\n",
    "\n",
    "An AI-powered lab report analyzer that helps Indian patients understand their blood test results in plain English.\n",
    "\n",
    "## Problem\n",
    "- Patients struggle to understand medical jargon in lab reports\n",
    "- Reference ranges are confusing without context\n",
    "- No easy way to track health trends over time\n",
    "\n",
    "## Solution\n",
    "Upload blood test report → Get plain English explanations for each value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q transformers accelerate pillow pdf2image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "from PIL import Image\n",
    "from transformers import AutoProcessor, AutoModelForImageTextToText\n",
    "from google.colab import userdata\n",
    "import os\n",
    "\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load MedGemma Model\n",
    "\n",
    "Using MedGemma 1.5 4B - Google's open-source medical AI model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Model configuration - MedGemma 1.5 (latest, Jan 2026)\nMODEL_ID = \"google/medgemma-1.5-4b-it\"\n\n# Get HF token from Kaggle secrets\n# Add your token in Kaggle: Settings > Secrets > Add \"HF_TOKEN\"\ntry:\n    HF_TOKEN = userdata.get('HF_TOKEN')\nexcept:\n    HF_TOKEN = os.environ.get('HF_TOKEN', None)\n\nif not HF_TOKEN:\n    print(\"⚠️ HF_TOKEN not found. Add it in Kaggle Secrets.\")\nelse:\n    print(\"✓ HF_TOKEN found\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load processor and model\n",
    "print(\"Loading MedGemma processor...\")\n",
    "processor = AutoProcessor.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    token=HF_TOKEN,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "print(\"Loading MedGemma model (this takes 2-3 minutes)...\")\n",
    "model = AutoModelForImageTextToText.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    token=HF_TOKEN,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "print(\"✓ MedGemma loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extraction Prompt\n",
    "\n",
    "Carefully crafted prompt for extracting lab values from Indian lab report formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXTRACTION_PROMPT = \"\"\"You are a medical lab report analyzer. Extract all test values from this lab report image.\n",
    "\n",
    "For each test, provide:\n",
    "- test_name: Name of the test (e.g., \"Hemoglobin\", \"Fasting Blood Sugar\", \"TSH\")\n",
    "- value: Numeric value as shown\n",
    "- unit: Unit of measurement (e.g., \"g/dL\", \"mg/dL\", \"mIU/L\")\n",
    "- reference_range: Normal range as shown on report\n",
    "- status: \"normal\", \"high\", or \"low\" based on reference range\n",
    "\n",
    "Return ONLY a JSON array. Example:\n",
    "[\n",
    "  {\"test_name\": \"Hemoglobin\", \"value\": 14.2, \"unit\": \"g/dL\", \"reference_range\": \"13.0-17.0\", \"status\": \"normal\"}\n",
    "]\n",
    "\n",
    "Extract ALL tests visible. Use exact values. Handle Indian lab formats (Thyrocare, SRL, Dr. Lal PathLabs).\n",
    "\"\"\"\n",
    "\n",
    "EXPLANATION_PROMPT = \"\"\"You are a friendly medical educator. Explain this lab value simply:\n",
    "\n",
    "Test: {test_name}\n",
    "Value: {value} {unit}\n",
    "Normal Range: {reference_range}\n",
    "Status: {status}\n",
    "\n",
    "In under 80 words, explain:\n",
    "1. What this test measures\n",
    "2. What your result means\n",
    "3. One actionable tip (if needed)\n",
    "\n",
    "Use simple language. Never diagnose - suggest discussing with doctor if abnormal.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Core Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_lab_values(image: Image.Image) -> list:\n",
    "    \"\"\"Extract lab values from a lab report image using MedGemma.\"\"\"\n",
    "    \n",
    "    # Resize if needed\n",
    "    max_size = 1024\n",
    "    if max(image.size) > max_size:\n",
    "        ratio = max_size / max(image.size)\n",
    "        new_size = (int(image.size[0] * ratio), int(image.size[1] * ratio))\n",
    "        image = image.resize(new_size, Image.Resampling.LANCZOS)\n",
    "    \n",
    "    # Prepare inputs\n",
    "    inputs = processor(\n",
    "        images=image,\n",
    "        text=EXTRACTION_PROMPT,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(model.device)\n",
    "    \n",
    "    # Generate\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=2048,\n",
    "            do_sample=False\n",
    "        )\n",
    "    \n",
    "    # Decode\n",
    "    response = processor.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Parse JSON\n",
    "    try:\n",
    "        start = response.find('[')\n",
    "        end = response.rfind(']') + 1\n",
    "        if start != -1 and end > start:\n",
    "            return json.loads(response[start:end])\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"JSON parsing error: {e}\")\n",
    "        print(f\"Raw response: {response}\")\n",
    "    \n",
    "    return []\n",
    "\n",
    "\n",
    "def explain_lab_value(test_name: str, value: float, unit: str, \n",
    "                      reference_range: str, status: str) -> str:\n",
    "    \"\"\"Generate plain English explanation for a lab value.\"\"\"\n",
    "    \n",
    "    prompt = EXPLANATION_PROMPT.format(\n",
    "        test_name=test_name,\n",
    "        value=value,\n",
    "        unit=unit,\n",
    "        reference_range=reference_range,\n",
    "        status=status\n",
    "    )\n",
    "    \n",
    "    inputs = processor(\n",
    "        text=prompt,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=200,\n",
    "            do_sample=True,\n",
    "            temperature=0.7\n",
    "        )\n",
    "    \n",
    "    response = processor.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Remove prompt from response\n",
    "    if prompt in response:\n",
    "        response = response.replace(prompt, \"\").strip()\n",
    "    \n",
    "    return response\n",
    "\n",
    "\n",
    "def analyze_report(image: Image.Image) -> dict:\n",
    "    \"\"\"Full analysis: extract values + generate explanations.\"\"\"\n",
    "    \n",
    "    print(\"Extracting lab values...\")\n",
    "    lab_values = extract_lab_values(image)\n",
    "    print(f\"Found {len(lab_values)} tests\")\n",
    "    \n",
    "    results = []\n",
    "    for i, val in enumerate(lab_values):\n",
    "        print(f\"Explaining {i+1}/{len(lab_values)}: {val.get('test_name', 'Unknown')}...\")\n",
    "        \n",
    "        explanation = explain_lab_value(\n",
    "            val.get('test_name', ''),\n",
    "            val.get('value', 0),\n",
    "            val.get('unit', ''),\n",
    "            val.get('reference_range', 'N/A'),\n",
    "            val.get('status', 'normal')\n",
    "        )\n",
    "        \n",
    "        results.append({\n",
    "            **val,\n",
    "            'explanation': explanation\n",
    "        })\n",
    "    \n",
    "    return {\n",
    "        'total_tests': len(results),\n",
    "        'normal': sum(1 for r in results if r.get('status') == 'normal'),\n",
    "        'abnormal': sum(1 for r in results if r.get('status') in ['high', 'low']),\n",
    "        'results': results\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test with Sample Lab Report\n",
    "\n",
    "Upload a lab report image to test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload sample lab report\n",
    "from google.colab import files\n",
    "import io\n",
    "\n",
    "print(\"Upload a lab report image (PNG, JPG) or PDF:\")\n",
    "uploaded = files.upload()\n",
    "\n",
    "for filename, content in uploaded.items():\n",
    "    print(f\"\\nProcessing: {filename}\")\n",
    "    \n",
    "    if filename.lower().endswith('.pdf'):\n",
    "        from pdf2image import convert_from_bytes\n",
    "        images = convert_from_bytes(content, first_page=1, last_page=1)\n",
    "        image = images[0]\n",
    "    else:\n",
    "        image = Image.open(io.BytesIO(content)).convert('RGB')\n",
    "    \n",
    "    print(f\"Image size: {image.size}\")\n",
    "    display(image.resize((400, int(400 * image.size[1] / image.size[0]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run analysis\n",
    "results = analyze_report(image)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ANALYSIS COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Total tests: {results['total_tests']}\")\n",
    "print(f\"Normal: {results['normal']}\")\n",
    "print(f\"Abnormal: {results['abnormal']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display results\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "def display_results(results):\n",
    "    html = \"<div style='font-family: Arial, sans-serif;'>\"\n",
    "    \n",
    "    for r in results['results']:\n",
    "        status = r.get('status', 'normal')\n",
    "        color = '#28a745' if status == 'normal' else '#dc3545' if status == 'high' else '#ffc107'\n",
    "        badge = '✓ Normal' if status == 'normal' else '↑ High' if status == 'high' else '↓ Low'\n",
    "        \n",
    "        html += f\"\"\"\n",
    "        <div style='border: 1px solid #ddd; border-left: 4px solid {color}; \n",
    "                    padding: 15px; margin: 10px 0; border-radius: 4px;'>\n",
    "            <div style='display: flex; justify-content: space-between; align-items: center;'>\n",
    "                <h3 style='margin: 0; color: #333;'>{r.get('test_name', 'Unknown')}</h3>\n",
    "                <span style='background: {color}; color: white; padding: 4px 12px; \n",
    "                             border-radius: 20px; font-size: 12px;'>{badge}</span>\n",
    "            </div>\n",
    "            <p style='font-size: 24px; margin: 10px 0; color: #333;'>\n",
    "                <strong>{r.get('value', 'N/A')}</strong> \n",
    "                <span style='font-size: 14px; color: #666;'>{r.get('unit', '')}</span>\n",
    "            </p>\n",
    "            <p style='color: #666; font-size: 13px; margin: 5px 0;'>\n",
    "                Reference: {r.get('reference_range', 'N/A')}\n",
    "            </p>\n",
    "            <hr style='border: none; border-top: 1px solid #eee; margin: 10px 0;'>\n",
    "            <p style='color: #444; line-height: 1.5;'>{r.get('explanation', 'No explanation available.')}</p>\n",
    "        </div>\n",
    "        \"\"\"\n",
    "    \n",
    "    html += \"</div>\"\n",
    "    display(HTML(html))\n",
    "\n",
    "display_results(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### What MedGemma Does Well\n",
    "- Extracts structured data from lab report images\n",
    "- Identifies test names, values, units, and reference ranges\n",
    "- Classifies values as normal/high/low\n",
    "- Generates patient-friendly explanations\n",
    "\n",
    "### Impact\n",
    "- Helps patients understand their health data\n",
    "- Reduces anxiety from confusing medical jargon\n",
    "- Empowers informed discussions with doctors\n",
    "\n",
    "### Next Steps\n",
    "- Add trend tracking (compare with previous reports)\n",
    "- Support more lab report formats\n",
    "- Build mobile-friendly web app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results to JSON\n",
    "with open('analysis_results.json', 'w') as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "print(\"Results saved to analysis_results.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}