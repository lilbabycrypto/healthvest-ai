{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MedGemma Lab Report Extraction Test\n",
    "\n",
    "Testing MedGemma 1.5 4B for lab report analysis.\n",
    "\n",
    "**Goal**: Validate that MedGemma can:\n",
    "1. Extract lab values from Indian lab report images\n",
    "2. Identify test names, values, units, and reference ranges\n",
    "3. Classify values as normal/high/low"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install transformers torch accelerate Pillow -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import AutoProcessor, AutoModelForVision2Seq\n",
    "import json\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"MPS available: {torch.backends.mps.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load MedGemma Model\n",
    "\n",
    "Using the 4B instruction-tuned variant for best results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model ID - MedGemma 1.5 4B\n",
    "MODEL_ID = \"google/medgemma-4b-it\"\n",
    "\n",
    "# Detect device\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "    dtype = torch.float16\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "    dtype = torch.float16\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "    dtype = torch.float32\n",
    "\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load processor and model\n",
    "print(\"Loading processor...\")\n",
    "processor = AutoProcessor.from_pretrained(MODEL_ID, trust_remote_code=True)\n",
    "\n",
    "print(\"Loading model (this may take a few minutes)...\")\n",
    "model = AutoModelForVision2Seq.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    torch_dtype=dtype,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "print(\"Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extraction Prompt\n",
    "\n",
    "Carefully crafted prompt for lab value extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXTRACTION_PROMPT = \"\"\"You are a medical lab report analyzer. Extract all test values from this lab report image.\n",
    "\n",
    "For each test, provide:\n",
    "- test_name: Name of the test (e.g., \"Hemoglobin\", \"Fasting Blood Sugar\", \"TSH\")\n",
    "- value: Numeric value as shown\n",
    "- unit: Unit of measurement (e.g., \"g/dL\", \"mg/dL\", \"mIU/L\")\n",
    "- reference_range: Normal range as shown on report\n",
    "- status: \"normal\", \"high\", or \"low\" based on reference range\n",
    "\n",
    "Return as a JSON array. Example format:\n",
    "[\n",
    "  {\n",
    "    \"test_name\": \"Hemoglobin\",\n",
    "    \"value\": 14.2,\n",
    "    \"unit\": \"g/dL\",\n",
    "    \"reference_range\": \"13.0 - 17.0\",\n",
    "    \"status\": \"normal\"\n",
    "  }\n",
    "]\n",
    "\n",
    "Important:\n",
    "- Extract ALL tests visible in the report\n",
    "- Use exact values as shown (don't round)\n",
    "- If reference range is missing, use \"N/A\"\n",
    "- Handle common Indian lab formats (Thyrocare, SRL, Dr. Lal PathLabs, Metropolis)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_lab_values(image_path: str) -> list:\n",
    "    \"\"\"Extract lab values from a lab report image.\"\"\"\n",
    "    \n",
    "    # Load image\n",
    "    image = Image.open(image_path)\n",
    "    if image.mode != 'RGB':\n",
    "        image = image.convert('RGB')\n",
    "    \n",
    "    # Resize if needed (MedGemma optimal: up to 1568x1568)\n",
    "    max_size = 1568\n",
    "    if max(image.size) > max_size:\n",
    "        ratio = max_size / max(image.size)\n",
    "        new_size = (int(image.size[0] * ratio), int(image.size[1] * ratio))\n",
    "        image = image.resize(new_size, Image.Resampling.LANCZOS)\n",
    "    \n",
    "    print(f\"Image size: {image.size}\")\n",
    "    \n",
    "    # Prepare inputs\n",
    "    inputs = processor(\n",
    "        images=image,\n",
    "        text=EXTRACTION_PROMPT,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(device)\n",
    "    \n",
    "    # Generate\n",
    "    print(\"Extracting lab values...\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=2048,\n",
    "            do_sample=False\n",
    "        )\n",
    "    \n",
    "    # Decode\n",
    "    response = processor.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Parse JSON\n",
    "    try:\n",
    "        start_idx = response.find('[')\n",
    "        end_idx = response.rfind(']') + 1\n",
    "        if start_idx != -1 and end_idx > start_idx:\n",
    "            json_str = response[start_idx:end_idx]\n",
    "            return json.loads(json_str)\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"JSON parsing error: {e}\")\n",
    "        print(f\"Raw response: {response}\")\n",
    "    \n",
    "    return []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test with Sample Lab Reports\n",
    "\n",
    "Upload your lab report images to test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with a sample image\n",
    "# Replace with your lab report image path\n",
    "TEST_IMAGE = \"/path/to/your/lab_report.jpg\"\n",
    "\n",
    "# Run extraction\n",
    "# results = extract_lab_values(TEST_IMAGE)\n",
    "# print(json.dumps(results, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explanation Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPLANATION_PROMPT = \"\"\"You are a friendly medical educator helping a patient understand their lab results.\n",
    "\n",
    "Test Information:\n",
    "- Test Name: {test_name}\n",
    "- Your Value: {value} {unit}\n",
    "- Normal Range: {reference_range}\n",
    "- Status: {status}\n",
    "\n",
    "Explain this in simple terms (under 100 words):\n",
    "1. What it measures\n",
    "2. What your result means\n",
    "3. One actionable tip\n",
    "\n",
    "Use everyday language, avoid medical jargon.\n",
    "\"\"\"\n",
    "\n",
    "def explain_lab_value(test_name, value, unit, reference_range, status):\n",
    "    \"\"\"Generate explanation for a lab value.\"\"\"\n",
    "    \n",
    "    prompt = EXPLANATION_PROMPT.format(\n",
    "        test_name=test_name,\n",
    "        value=value,\n",
    "        unit=unit,\n",
    "        reference_range=reference_range,\n",
    "        status=status\n",
    "    )\n",
    "    \n",
    "    inputs = processor(\n",
    "        text=prompt,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=256,\n",
    "            do_sample=True,\n",
    "            temperature=0.7\n",
    "        )\n",
    "    \n",
    "    response = processor.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Remove prompt from response\n",
    "    if prompt in response:\n",
    "        response = response.replace(prompt, \"\").strip()\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test explanation generation\n",
    "# explanation = explain_lab_value(\n",
    "#     test_name=\"Hemoglobin\",\n",
    "#     value=12.5,\n",
    "#     unit=\"g/dL\",\n",
    "#     reference_range=\"13.0 - 17.0\",\n",
    "#     status=\"low\"\n",
    "# )\n",
    "# print(explanation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "1. Test with 3+ real lab report images\n",
    "2. Measure extraction accuracy\n",
    "3. Tune prompts if needed\n",
    "4. Integrate into FastAPI backend"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
